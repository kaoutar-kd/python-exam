{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "literary-population",
   "metadata": {},
   "source": [
    "Fait par : El Keddadi Kaoutar et Bentaleb Aya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "amber-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "prepared-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int\n",
    "        Number of incomming connections.\n",
    "    \n",
    "    L_out : int\n",
    "        Number of outgoing connections. \n",
    "    \n",
    "    epsilon_init : float, optional\n",
    "        Range of values which the weight can take from a uniform \n",
    "        distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like\n",
    "        The weight initialiatized to random values.  Note that W should\n",
    "        be set to a matrix of size(L_out, 1 + L_in) as\n",
    "        the first column of W handles the \"bias\" terms.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Initialize W randomly so that we break the symmetry while training\n",
    "    the neural network. Note that the first column of W corresponds \n",
    "    to the parameters for the bias unit.\n",
    "    \"\"\"\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "meaning-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = randInitializeWeights(400, 18)\n",
    "initial_Theta2 = randInitializeWeights(18,18)\n",
    "initial_Theta3 = randInitializeWeights(18, 10)\n",
    "nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel(),initial_Theta3.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "reverse-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    This should work regardless if z is a matrix or a vector. \n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the gradient of the sigmoid function evaluated at\n",
    "    each value of z (z can be a matrix, vector or scalar).\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    We have provided an implementation of the sigmoid function \n",
    "    in `utils.py` file accompanying this assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    g = utils.sigmoid(z) * (1 - utils.sigmoid(z))\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "formed-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20x20 Input Images of Digits\n",
    "input_layer_size  = 400\n",
    "\n",
    "hidden_layer_size = 18\n",
    "num_labels = 10\n",
    "\n",
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join(r'Data', 'ex4data1.mat'))\n",
    "\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "\n",
    "y[y == 10] = 0\n",
    "m = y.size\n",
    "d=np.concatenate((X,y.reshape(5000,1)), axis=1)\n",
    "data=np.split(d,20,axis=0)\n",
    "data_test=data[12]\n",
    "data_etude=np.concatenate((data[0],data[1],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10],data[11],data[13],data[14],data[15],data[16],data[17],data[18],data[19]), axis=0)\n",
    "data_etude.shape\n",
    "\n",
    "X = data_etude[:,:400]\n",
    "y_f = data_etude[:,-1].reshape(4500)\n",
    "y=y[:4500]\n",
    "layers=[400,18,18,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "married-casting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "neutral-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,Layers,X, y, lambda_=0.0):\n",
    "    \n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:Layers[1] * (Layers[0] + 1)],(Layers[1], (Layers[0] + 1)))\n",
    "    nn_params = nn_params[Layers[1] * (Layers[0] + 1):]\n",
    "    Theta2 = np.reshape(nn_params[:Layers[2] * (Layers[1] + 1)],(Layers[2], (Layers[1] + 1)))\n",
    "    Theta3 = np.reshape(nn_params[Layers[2] * (Layers[1] + 1):],(Layers[3], (Layers[2] + 1)))\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    Theta2_grad = np.zeros(Theta3.shape)\n",
    "    \n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    \n",
    "    a2 = utils.sigmoid(a1.dot(Theta1.T))\n",
    "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n",
    "    \n",
    "    a3 = utils.sigmoid(a2.dot(Theta2.T))\n",
    "    a3 = np.concatenate([np.ones((a3.shape[0], 1)), a3], axis=1)\n",
    "    \n",
    "    a4 = utils.sigmoid(a3.dot(Theta3.T))\n",
    "    \n",
    "    y_matrix = np.eye(Layers[-1])[y]\n",
    "    p=np.argmax(a4, axis=1)\n",
    "    print('%.2f' % (np.mean(p== y) * 100))\n",
    "    temp1 = Theta1\n",
    "    temp2 = Theta2\n",
    "    temp3 = Theta3\n",
    "    \n",
    "    # Add regularization term\n",
    "    \n",
    "    reg_term = (lambda_ / (2 * m)) * (np.sum(np.square(temp1[:, 1:])) + np.sum(np.square(temp2[:, 1:])) + + np.sum(np.square(temp3[:, 1:])))\n",
    "    \n",
    "    J = (-1 / m) * np.sum((np.log(a4) * y_matrix) + np.log(1 - a4) * (1 - y_matrix)) + reg_term\n",
    "    \n",
    "    # Backpropogation\n",
    "    \n",
    "    delta_4 = a4 - y_matrix\n",
    "    delta_3 = delta_4.dot(Theta3)[:, 1:] * sigmoidGradient(a2.dot(Theta2.T))\n",
    "    delta_2 = delta_3.dot(Theta2)[:, 1:] * sigmoidGradient(a1.dot(Theta1.T))\n",
    "\n",
    "    Delta1 = delta_2.T.dot(a1)\n",
    "    Delta2 = delta_3.T.dot(a2)\n",
    "    Delta3 = delta_4.T.dot(a3)\n",
    "    \n",
    "    # Add regularization to gradient\n",
    "\n",
    "    Theta1_grad = (1 / m) * Delta1\n",
    "    Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n",
    "    \n",
    "    Theta2_grad = (1 / m) * Delta2\n",
    "    Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n",
    "    \n",
    "    Theta3_grad = (1 / m) * Delta3\n",
    "    Theta3_grad[:, 1:] = Theta3_grad[:, 1:] + (lambda_ / m) * Theta3[:, 1:]\n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel(), Theta3_grad.ravel()])\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "simple-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 401) (18, 19) (10, 19)\n",
      "Cost at parameters (loaded from ex4weights): 5.881377 \n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(nn_params, layers, X, y, lambda_)\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f ' % J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "declared-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.33\n",
      "41.80\n",
      "29.38\n",
      "27.93\n",
      "39.71\n",
      "42.07\n",
      "44.60\n",
      "44.98\n",
      "46.69\n",
      "47.51\n",
      "47.04\n",
      "47.62\n",
      "50.44\n",
      "54.20\n",
      "55.62\n",
      "57.22\n",
      "57.82\n",
      "58.76\n",
      "59.44\n",
      "59.76\n",
      "61.33\n",
      "60.98\n",
      "61.40\n",
      "60.89\n",
      "61.67\n",
      "63.22\n",
      "63.09\n",
      "62.49\n",
      "62.62\n",
      "63.42\n",
      "63.47\n",
      "63.07\n",
      "64.11\n",
      "63.78\n",
      "63.91\n",
      "64.76\n",
      "64.36\n",
      "64.20\n",
      "64.04\n",
      "65.22\n",
      "64.80\n",
      "64.42\n",
      "64.38\n",
      "64.67\n",
      "65.31\n",
      "65.27\n",
      "64.69\n",
      "65.13\n",
      "64.78\n",
      "64.62\n",
      "64.93\n",
      "65.84\n",
      "65.64\n",
      "65.89\n",
      "65.58\n",
      "64.98\n",
      "65.16\n",
      "65.62\n",
      "66.29\n",
      "66.02\n",
      "65.96\n",
      "65.42\n",
      "65.64\n",
      "66.22\n",
      "67.87\n",
      "68.40\n",
      "67.89\n",
      "67.09\n",
      "66.47\n",
      "67.04\n",
      "67.24\n",
      "67.40\n",
      "68.36\n",
      "68.49\n",
      "68.31\n",
      "67.71\n",
      "67.87\n",
      "68.64\n",
      "68.44\n",
      "68.33\n",
      "68.18\n",
      "69.11\n",
      "69.16\n",
      "69.49\n",
      "70.42\n",
      "70.29\n",
      "70.87\n",
      "70.96\n",
      "70.89\n",
      "68.87\n",
      "69.78\n",
      "70.11\n",
      "70.62\n",
      "71.58\n",
      "71.62\n",
      "72.20\n",
      "72.87\n",
      "72.69\n",
      "72.40\n",
      "71.24\n",
      "72.56\n",
      "73.53\n",
      "73.11\n",
      "72.82\n",
      "72.64\n",
      "72.80\n",
      "72.56\n",
      "72.40\n",
      "72.36\n",
      "73.33\n",
      "73.07\n",
      "72.96\n",
      "72.96\n",
      "72.24\n",
      "72.60\n",
      "73.02\n",
      "74.09\n",
      "73.73\n",
      "73.27\n",
      "72.93\n",
      "73.04\n",
      "73.47\n",
      "73.36\n",
      "73.40\n",
      "73.69\n",
      "74.18\n",
      "74.76\n",
      "74.71\n",
      "74.38\n",
      "74.47\n",
      "74.58\n",
      "74.51\n",
      "74.93\n",
      "75.71\n",
      "75.91\n",
      "75.29\n",
      "75.36\n",
      "75.89\n",
      "76.40\n",
      "76.38\n",
      "76.40\n",
      "76.40\n",
      "76.36\n",
      "76.56\n",
      "76.64\n",
      "76.76\n",
      "76.69\n",
      "76.73\n",
      "76.78\n",
      "76.69\n",
      "76.76\n",
      "76.89\n",
      "76.98\n",
      "77.40\n",
      "77.62\n",
      "77.67\n",
      "77.49\n",
      "77.67\n",
      "77.71\n",
      "78.09\n",
      "77.78\n",
      "77.93\n",
      "78.31\n",
      "78.09\n",
      "78.36\n",
      "78.04\n",
      "78.13\n",
      "78.29\n",
      "78.40\n",
      "78.29\n",
      "78.56\n",
      "78.51\n",
      "78.56\n",
      "78.56\n",
      "78.62\n",
      "78.71\n",
      "78.69\n",
      "78.67\n",
      "78.67\n",
      "78.82\n",
      "78.89\n",
      "78.96\n",
      "79.11\n",
      "79.04\n",
      "79.07\n",
      "79.40\n",
      "79.27\n",
      "79.33\n",
      "79.04\n",
      "79.09\n",
      "79.09\n",
      "79.22\n",
      "79.31\n",
      "79.47\n",
      "79.47\n",
      "79.38\n",
      "79.42\n",
      "79.53\n",
      "79.51\n",
      "79.78\n",
      "79.58\n",
      "79.64\n",
      "79.60\n",
      "79.64\n",
      "79.80\n",
      "79.80\n",
      "79.89\n",
      "79.76\n",
      "79.80\n",
      "79.87\n",
      "79.84\n",
      "79.76\n",
      "79.93\n",
      "79.84\n",
      "79.96\n",
      "80.00\n",
      "79.91\n",
      "79.84\n",
      "79.87\n",
      "79.93\n",
      "80.02\n",
      "80.13\n",
      "79.93\n",
      "80.36\n",
      "80.44\n",
      "80.44\n",
      "80.33\n",
      "80.40\n",
      "80.38\n",
      "80.53\n",
      "80.53\n",
      "80.60\n",
      "80.73\n",
      "80.71\n",
      "80.89\n",
      "80.80\n",
      "80.71\n",
      "80.80\n",
      "80.91\n",
      "80.73\n",
      "80.82\n",
      "80.73\n",
      "80.73\n",
      "80.73\n",
      "80.84\n",
      "80.87\n",
      "80.93\n",
      "80.80\n",
      "80.78\n",
      "80.76\n",
      "81.11\n",
      "81.27\n",
      "81.02\n",
      "81.09\n",
      "81.07\n",
      "81.24\n",
      "81.18\n",
      "81.02\n",
      "80.89\n",
      "81.07\n",
      "81.38\n",
      "81.56\n",
      "81.76\n",
      "81.56\n",
      "81.58\n",
      "81.69\n",
      "81.62\n",
      "81.47\n",
      "81.47\n",
      "81.69\n",
      "81.40\n",
      "81.49\n",
      "81.44\n",
      "81.42\n",
      "81.44\n",
      "81.44\n",
      "81.56\n",
      "81.51\n",
      "81.49\n",
      "81.58\n",
      "81.64\n",
      "81.78\n",
      "81.73\n",
      "81.73\n",
      "81.73\n",
      "81.69\n",
      "81.71\n",
      "81.84\n",
      "81.80\n",
      "81.76\n",
      "81.91\n",
      "81.93\n",
      "82.00\n",
      "81.82\n",
      "81.93\n",
      "81.98\n",
      "81.89\n",
      "81.96\n",
      "81.98\n",
      "82.04\n",
      "81.98\n",
      "81.96\n",
      "82.04\n",
      "82.00\n",
      "82.07\n",
      "82.09\n",
      "81.96\n",
      "81.96\n",
      "82.02\n",
      "82.18\n",
      "82.11\n",
      "82.02\n",
      "82.22\n",
      "81.87\n",
      "82.27\n",
      "82.04\n",
      "82.00\n",
      "82.31\n",
      "82.31\n",
      "82.58\n",
      "82.40\n",
      "82.40\n",
      "82.33\n",
      "82.58\n",
      "82.42\n",
      "82.40\n",
      "82.31\n",
      "82.18\n",
      "82.13\n",
      "82.20\n",
      "82.40\n",
      "82.29\n",
      "82.40\n",
      "82.24\n",
      "82.36\n",
      "82.38\n",
      "82.36\n",
      "82.44\n",
      "82.60\n",
      "82.51\n",
      "82.82\n",
      "82.71\n",
      "82.53\n",
      "82.62\n",
      "82.67\n",
      "82.58\n",
      "82.49\n",
      "82.56\n",
      "82.71\n",
      "82.64\n",
      "82.56\n",
      "82.58\n",
      "82.69\n",
      "82.80\n",
      "82.78\n",
      "82.78\n",
      "82.73\n",
      "82.82\n",
      "82.89\n",
      "82.84\n",
      "82.84\n",
      "82.78\n",
      "82.78\n",
      "82.78\n",
      "82.80\n",
      "82.82\n",
      "82.84\n",
      "82.91\n",
      "82.93\n",
      "82.96\n",
      "82.80\n",
      "82.87\n",
      "82.84\n",
      "82.84\n",
      "83.02\n",
      "83.02\n",
      "83.13\n",
      "82.87\n",
      "83.29\n",
      "83.40\n",
      "83.40\n",
      "83.69\n",
      "83.76\n",
      "83.73\n",
      "83.84\n",
      "83.78\n",
      "83.73\n",
      "83.87\n",
      "83.64\n",
      "83.76\n",
      "83.84\n",
      "83.82\n",
      "83.84\n",
      "83.82\n",
      "83.84\n",
      "83.89\n",
      "83.87\n",
      "83.87\n",
      "83.82\n",
      "83.89\n",
      "83.96\n",
      "83.93\n",
      "84.04\n",
      "84.04\n",
      "84.04\n",
      "84.09\n",
      "84.11\n",
      "84.18\n",
      "84.16\n",
      "84.09\n",
      "84.04\n",
      "84.00\n",
      "84.09\n",
      "84.13\n",
      "84.22\n",
      "84.00\n",
      "83.93\n",
      "84.02\n",
      "84.02\n",
      "84.07\n",
      "84.16\n",
      "84.13\n",
      "84.29\n",
      "84.20\n",
      "84.00\n",
      "84.18\n",
      "84.24\n",
      "84.22\n",
      "84.20\n",
      "84.09\n",
      "84.29\n",
      "84.31\n",
      "84.24\n",
      "84.27\n",
      "84.27\n",
      "84.27\n",
      "84.27\n",
      "84.31\n",
      "84.36\n",
      "84.44\n",
      "84.42\n",
      "84.40\n",
      "84.31\n",
      "84.38\n",
      "84.33\n",
      "84.27\n",
      "84.47\n",
      "84.44\n",
      "84.40\n",
      "84.40\n",
      "84.44\n",
      "84.38\n",
      "84.36\n",
      "84.44\n",
      "84.51\n",
      "84.44\n",
      "84.29\n",
      "84.44\n",
      "84.47\n",
      "84.38\n",
      "84.36\n",
      "84.49\n",
      "84.53\n",
      "84.40\n",
      "84.49\n",
      "84.40\n",
      "84.36\n",
      "84.38\n",
      "84.42\n",
      "84.44\n",
      "84.42\n",
      "84.36\n",
      "84.47\n",
      "84.44\n",
      "84.42\n",
      "84.56\n",
      "84.49\n",
      "84.44\n",
      "84.49\n",
      "84.42\n",
      "84.36\n",
      "84.36\n",
      "84.36\n",
      "84.62\n",
      "84.44\n",
      "84.53\n",
      "84.56\n",
      "84.60\n",
      "84.64\n",
      "84.58\n",
      "84.64\n",
      "84.56\n",
      "84.62\n",
      "84.58\n",
      "84.62\n",
      "84.64\n",
      "84.58\n",
      "84.64\n",
      "84.60\n",
      "84.60\n",
      "84.76\n",
      "84.64\n",
      "84.69\n",
      "84.73\n",
      "84.64\n",
      "84.82\n",
      "84.76\n",
      "84.73\n",
      "84.67\n",
      "84.71\n",
      "84.73\n",
      "84.78\n",
      "84.76\n",
      "84.76\n",
      "84.73\n",
      "84.78\n",
      "84.76\n",
      "84.73\n",
      "84.73\n",
      "84.82\n",
      "84.84\n",
      "84.91\n",
      "84.89\n",
      "84.89\n",
      "84.87\n",
      "84.82\n",
      "84.84\n",
      "84.96\n",
      "84.93\n",
      "84.91\n",
      "84.93\n",
      "85.00\n",
      "85.00\n",
      "84.91\n",
      "84.87\n",
      "84.93\n",
      "85.07\n",
      "85.13\n",
      "85.09\n",
      "84.93\n",
      "85.07\n",
      "85.04\n",
      "85.18\n",
      "85.16\n",
      "85.13\n",
      "85.11\n",
      "85.09\n",
      "85.07\n",
      "85.09\n",
      "84.96\n",
      "85.02\n",
      "85.11\n",
      "85.16\n",
      "85.11\n",
      "85.13\n",
      "85.16\n",
      "85.16\n",
      "85.02\n",
      "85.07\n",
      "85.09\n",
      "85.11\n",
      "85.18\n",
      "85.18\n",
      "85.11\n",
      "85.18\n",
      "85.29\n",
      "85.31\n",
      "85.33\n",
      "85.31\n",
      "85.18\n",
      "85.13\n",
      "85.31\n",
      "85.40\n",
      "85.42\n",
      "85.40\n",
      "85.53\n",
      "85.51\n",
      "85.40\n",
      "85.40\n",
      "85.42\n",
      "85.42\n",
      "85.60\n",
      "85.62\n",
      "85.60\n",
      "85.60\n",
      "85.56\n",
      "85.47\n",
      "85.49\n",
      "85.49\n",
      "85.47\n",
      "85.36\n",
      "85.51\n",
      "85.51\n",
      "85.44\n",
      "85.44\n",
      "85.49\n",
      "85.40\n",
      "85.49\n",
      "85.51\n",
      "85.56\n",
      "85.60\n",
      "85.60\n",
      "85.53\n",
      "85.62\n",
      "85.73\n",
      "85.73\n",
      "85.71\n",
      "85.76\n",
      "85.71\n",
      "85.60\n",
      "85.73\n",
      "85.49\n",
      "85.64\n",
      "85.69\n",
      "85.67\n",
      "85.69\n",
      "85.62\n",
      "85.76\n",
      "85.78\n",
      "85.71\n",
      "85.73\n",
      "85.73\n",
      "85.80\n",
      "85.84\n",
      "85.91\n",
      "85.89\n",
      "85.78\n",
      "85.87\n",
      "85.87\n",
      "85.87\n",
      "85.87\n",
      "85.87\n",
      "85.80\n",
      "85.71\n",
      "85.84\n",
      "85.82\n",
      "85.78\n",
      "85.80\n",
      "85.89\n",
      "85.96\n",
      "85.98\n",
      "86.00\n",
      "85.98\n",
      "86.00\n",
      "85.89\n",
      "85.96\n",
      "85.93\n",
      "85.93\n",
      "85.91\n",
      "85.84\n",
      "85.87\n",
      "85.87\n",
      "85.78\n",
      "85.93\n",
      "85.82\n",
      "85.98\n",
      "86.00\n",
      "86.00\n",
      "85.98\n",
      "86.07\n",
      "85.98\n",
      "85.82\n",
      "85.96\n",
      "85.96\n",
      "86.00\n",
      "85.93\n",
      "85.96\n",
      "85.89\n",
      "85.89\n",
      "85.91\n",
      "85.98\n",
      "85.91\n",
      "86.04\n",
      "86.09\n",
      "86.13\n",
      "86.00\n",
      "86.00\n",
      "86.09\n",
      "86.24\n",
      "86.07\n",
      "86.16\n",
      "86.31\n",
      "86.13\n",
      "86.20\n",
      "86.22\n",
      "86.29\n",
      "86.24\n",
      "86.24\n",
      "86.33\n",
      "86.33\n",
      "86.31\n",
      "86.38\n",
      "86.36\n",
      "86.38\n",
      "86.44\n",
      "86.29\n",
      "86.31\n",
      "86.36\n",
      "86.38\n",
      "86.40\n",
      "86.40\n",
      "86.49\n",
      "86.44\n",
      "86.42\n",
      "86.44\n",
      "86.51\n",
      "86.51\n",
      "86.49\n",
      "86.49\n",
      "86.51\n",
      "86.51\n",
      "86.49\n",
      "86.47\n",
      "86.51\n",
      "86.58\n",
      "86.56\n",
      "86.47\n",
      "86.53\n",
      "86.47\n",
      "86.47\n",
      "86.60\n",
      "86.53\n",
      "86.49\n",
      "86.49\n",
      "86.47\n",
      "86.56\n",
      "86.38\n",
      "86.44\n",
      "86.49\n",
      "86.51\n",
      "86.47\n",
      "86.49\n",
      "86.51\n",
      "86.49\n",
      "86.47\n",
      "86.51\n",
      "86.49\n",
      "86.49\n",
      "86.51\n",
      "86.51\n",
      "86.58\n",
      "86.53\n",
      "86.53\n",
      "86.62\n",
      "86.64\n",
      "86.62\n",
      "86.62\n",
      "86.64\n",
      "86.62\n",
      "86.64\n",
      "86.67\n",
      "86.62\n",
      "86.62\n",
      "86.69\n",
      "86.67\n",
      "86.67\n",
      "86.62\n",
      "86.62\n",
      "86.69\n",
      "86.67\n",
      "86.73\n",
      "86.69\n",
      "86.62\n",
      "86.67\n",
      "86.67\n",
      "86.73\n",
      "86.69\n",
      "86.64\n",
      "86.64\n",
      "86.69\n",
      "86.73\n",
      "86.80\n",
      "86.80\n",
      "86.73\n",
      "86.71\n",
      "86.82\n",
      "86.73\n",
      "86.76\n",
      "86.84\n",
      "87.09\n",
      "87.24\n",
      "87.16\n",
      "87.36\n",
      "87.27\n",
      "87.36\n",
      "87.40\n",
      "87.49\n",
      "87.42\n",
      "87.56\n",
      "87.49\n",
      "87.31\n",
      "87.42\n",
      "87.51\n",
      "87.47\n",
      "87.38\n",
      "87.36\n",
      "87.40\n",
      "87.40\n",
      "87.42\n",
      "87.42\n",
      "87.38\n",
      "87.40\n",
      "87.49\n",
      "87.42\n",
      "87.44\n",
      "87.36\n",
      "87.44\n",
      "87.33\n",
      "87.31\n",
      "87.29\n",
      "87.33\n",
      "87.29\n",
      "87.38\n",
      "87.38\n",
      "87.38\n",
      "87.42\n",
      "87.38\n",
      "87.38\n",
      "87.31\n",
      "87.29\n",
      "87.36\n",
      "87.36\n",
      "87.38\n",
      "87.42\n",
      "87.42\n",
      "87.42\n",
      "87.42\n",
      "87.47\n",
      "87.40\n",
      "87.42\n",
      "87.38\n",
      "87.49\n",
      "87.40\n",
      "87.47\n",
      "87.42\n",
      "87.40\n",
      "87.44\n",
      "87.31\n",
      "87.38\n",
      "87.38\n",
      "87.31\n",
      "87.31\n",
      "87.38\n",
      "87.42\n",
      "87.40\n",
      "87.40\n",
      "87.31\n",
      "87.33\n",
      "87.42\n",
      "87.42\n",
      "87.38\n",
      "87.36\n",
      "87.47\n",
      "87.49\n",
      "87.42\n",
      "87.49\n",
      "87.49\n",
      "87.51\n",
      "87.49\n",
      "87.47\n",
      "87.49\n",
      "87.56\n",
      "87.56\n",
      "87.53\n",
      "87.58\n",
      "87.53\n",
      "87.58\n",
      "87.58\n",
      "87.58\n",
      "87.53\n",
      "87.56\n",
      "87.56\n",
      "87.60\n",
      "87.60\n",
      "87.56\n",
      "87.58\n",
      "87.60\n",
      "87.44\n",
      "87.56\n",
      "87.58\n",
      "87.58\n",
      "87.56\n",
      "87.49\n",
      "87.49\n",
      "87.60\n",
      "87.56\n",
      "87.60\n",
      "87.56\n",
      "87.58\n",
      "87.56\n",
      "87.56\n",
      "87.60\n",
      "87.62\n",
      "87.60\n",
      "87.51\n",
      "87.53\n",
      "87.49\n",
      "87.60\n",
      "87.56\n",
      "87.73\n",
      "87.67\n",
      "87.76\n",
      "87.71\n",
      "87.67\n",
      "87.60\n",
      "87.64\n",
      "87.69\n",
      "87.71\n",
      "87.71\n",
      "87.69\n",
      "87.71\n",
      "87.73\n",
      "87.53\n",
      "87.62\n",
      "87.62\n",
      "87.58\n",
      "87.67\n",
      "87.56\n",
      "87.64\n",
      "87.60\n",
      "87.60\n",
      "87.62\n",
      "87.67\n",
      "87.67\n",
      "87.62\n",
      "87.69\n",
      "87.62\n",
      "87.67\n",
      "87.69\n",
      "87.64\n",
      "87.64\n",
      "87.62\n",
      "87.67\n",
      "87.67\n",
      "87.73\n",
      "87.69\n",
      "87.71\n",
      "87.67\n",
      "87.67\n",
      "87.64\n",
      "87.67\n",
      "87.67\n",
      "87.69\n",
      "87.69\n",
      "87.71\n",
      "87.67\n",
      "87.64\n",
      "87.56\n",
      "87.53\n",
      "87.60\n",
      "87.53\n",
      "87.60\n",
      "87.73\n",
      "87.69\n",
      "87.73\n",
      "87.67\n",
      "87.73\n",
      "87.67\n",
      "87.67\n",
      "87.73\n",
      "87.69\n",
      "87.69\n",
      "87.73\n",
      "87.67\n",
      "87.73\n",
      "87.69\n",
      "87.64\n",
      "87.67\n",
      "87.69\n",
      "87.80\n",
      "87.73\n",
      "87.64\n",
      "87.69\n",
      "87.69\n",
      "87.73\n",
      "87.67\n",
      "87.69\n",
      "87.71\n",
      "87.71\n",
      "87.64\n",
      "87.64\n",
      "87.58\n",
      "87.53\n",
      "87.62\n",
      "87.62\n",
      "87.73\n",
      "87.67\n",
      "87.76\n",
      "87.71\n",
      "87.69\n",
      "87.67\n",
      "87.67\n",
      "87.73\n",
      "87.73\n",
      "87.73\n",
      "87.71\n",
      "87.76\n",
      "87.76\n",
      "87.69\n",
      "87.60\n",
      "87.78\n",
      "87.78\n",
      "87.76\n",
      "87.69\n",
      "87.67\n",
      "87.73\n",
      "87.67\n",
      "87.69\n",
      "87.71\n",
      "87.73\n",
      "87.76\n",
      "87.71\n",
      "87.67\n",
      "87.69\n",
      "87.76\n",
      "87.78\n",
      "87.73\n",
      "87.80\n",
      "87.73\n",
      "87.71\n",
      "87.76\n",
      "87.71\n",
      "87.69\n",
      "87.71\n",
      "87.71\n",
      "87.64\n",
      "87.56\n",
      "87.60\n",
      "87.67\n",
      "87.78\n",
      "87.73\n",
      "87.71\n",
      "87.80\n",
      "87.80\n",
      "87.76\n",
      "87.71\n",
      "87.80\n",
      "87.78\n",
      "87.78\n",
      "87.78\n",
      "87.82\n",
      "87.78\n",
      "87.78\n",
      "87.76\n",
      "87.76\n",
      "87.71\n",
      "87.69\n",
      "87.71\n",
      "87.69\n",
      "87.71\n",
      "87.76\n",
      "87.80\n",
      "87.82\n",
      "87.84\n",
      "87.84\n",
      "87.69\n",
      "87.78\n",
      "87.76\n",
      "87.78\n",
      "87.78\n",
      "87.80\n",
      "87.80\n",
      "87.73\n",
      "87.78\n",
      "87.78\n",
      "87.76\n",
      "87.76\n",
      "87.80\n",
      "87.78\n",
      "87.71\n",
      "87.78\n",
      "87.80\n",
      "87.87\n",
      "87.87\n",
      "87.71\n",
      "87.71\n",
      "87.71\n",
      "87.62\n",
      "87.58\n",
      "87.73\n",
      "87.73\n",
      "87.84\n",
      "87.82\n",
      "87.84\n",
      "87.91\n",
      "87.84\n",
      "87.93\n",
      "88.04\n",
      "88.02\n",
      "88.02\n",
      "88.07\n",
      "88.07\n",
      "88.07\n",
      "88.09\n",
      "88.07\n",
      "88.09\n",
      "88.04\n",
      "88.00\n",
      "88.04\n",
      "88.02\n",
      "88.00\n",
      "88.04\n",
      "87.96\n",
      "88.04\n",
      "88.04\n",
      "87.98\n",
      "88.02\n",
      "88.02\n",
      "87.96\n",
      "88.04\n",
      "88.07\n",
      "87.93\n",
      "88.00\n",
      "88.00\n",
      "88.02\n",
      "88.02\n",
      "88.02\n",
      "88.04\n",
      "88.02\n",
      "88.02\n",
      "87.98\n",
      "87.96\n",
      "87.93\n",
      "87.93\n",
      "87.93\n",
      "87.96\n",
      "87.98\n",
      "87.98\n",
      "88.00\n",
      "87.98\n",
      "88.02\n",
      "88.02\n",
      "87.98\n",
      "88.02\n",
      "87.93\n",
      "87.89\n",
      "87.91\n",
      "87.91\n",
      "87.96\n",
      "87.96\n",
      "87.93\n",
      "87.91\n",
      "88.04\n",
      "88.00\n",
      "87.91\n",
      "87.93\n",
      "87.89\n",
      "87.91\n",
      "87.91\n",
      "87.96\n",
      "87.87\n",
      "87.89\n",
      "87.91\n",
      "88.02\n",
      "88.00\n",
      "87.96\n",
      "87.91\n",
      "87.84\n",
      "87.93\n",
      "87.91\n",
      "88.09\n",
      "88.13\n",
      "88.29\n",
      "88.24\n",
      "88.13\n",
      "88.24\n",
      "88.20\n",
      "88.24\n",
      "88.22\n",
      "88.11\n",
      "88.18\n",
      "88.20\n",
      "88.13\n",
      "88.16\n",
      "88.13\n",
      "88.16\n",
      "88.18\n",
      "88.18\n",
      "88.13\n",
      "88.07\n",
      "88.11\n",
      "88.18\n",
      "88.18\n",
      "88.20\n",
      "88.18\n",
      "88.16\n",
      "88.16\n",
      "88.13\n",
      "88.13\n",
      "88.09\n",
      "88.13\n",
      "88.18\n",
      "88.18\n",
      "88.20\n",
      "88.24\n",
      "88.07\n",
      "88.18\n",
      "88.16\n",
      "88.20\n",
      "88.16\n",
      "88.13\n",
      "88.09\n",
      "88.11\n",
      "88.11\n",
      "88.16\n",
      "88.16\n",
      "88.09\n",
      "88.09\n",
      "88.16\n",
      "88.02\n",
      "88.02\n",
      "88.02\n",
      "88.13\n",
      "88.20\n",
      "88.16\n",
      "88.11\n",
      "88.09\n",
      "88.04\n",
      "88.11\n",
      "88.13\n",
      "88.09\n",
      "88.04\n",
      "88.09\n",
      "88.11\n",
      "88.11\n",
      "88.11\n",
      "88.09\n",
      "88.11\n",
      "88.22\n",
      "88.16\n",
      "88.13\n",
      "88.22\n",
      "88.13\n",
      "88.13\n",
      "88.11\n",
      "88.09\n",
      "88.18\n",
      "88.24\n",
      "88.22\n",
      "88.11\n",
      "88.22\n",
      "88.18\n",
      "88.22\n",
      "88.27\n",
      "88.07\n",
      "88.13\n",
      "88.18\n",
      "88.22\n",
      "88.11\n",
      "88.18\n",
      "88.16\n",
      "88.13\n",
      "88.29\n",
      "88.11\n",
      "88.16\n",
      "88.20\n",
      "88.29\n",
      "88.27\n",
      "88.29\n",
      "88.20\n",
      "88.33\n",
      "88.20\n",
      "88.20\n",
      "88.18\n",
      "88.16\n",
      "88.20\n",
      "88.22\n",
      "88.22\n",
      "88.22\n",
      "88.24\n",
      "88.22\n",
      "88.20\n",
      "88.18\n",
      "88.18\n",
      "88.13\n",
      "88.29\n",
      "88.31\n",
      "88.22\n",
      "88.13\n",
      "88.16\n",
      "88.18\n",
      "88.18\n",
      "88.16\n",
      "88.02\n",
      "88.13\n",
      "88.00\n",
      "88.09\n",
      "88.09\n",
      "88.09\n",
      "88.00\n",
      "88.04\n",
      "88.04\n",
      "88.02\n",
      "88.07\n",
      "88.11\n",
      "88.11\n",
      "88.07\n",
      "88.16\n",
      "88.04\n",
      "88.16\n",
      "88.11\n",
      "88.09\n",
      "88.09\n",
      "88.22\n",
      "88.16\n",
      "88.20\n",
      "88.24\n",
      "88.16\n",
      "88.20\n",
      "88.16\n",
      "88.22\n",
      "88.27\n",
      "88.16\n",
      "88.18\n",
      "88.20\n",
      "88.20\n",
      "88.18\n",
      "88.16\n",
      "88.20\n",
      "88.20\n",
      "88.24\n",
      "88.20\n",
      "88.20\n",
      "88.24\n",
      "88.22\n",
      "88.11\n",
      "88.20\n",
      "88.27\n",
      "88.29\n",
      "88.31\n",
      "88.24\n",
      "88.16\n",
      "88.27\n",
      "88.22\n",
      "88.27\n",
      "88.20\n",
      "88.22\n",
      "88.22\n",
      "88.33\n",
      "88.27\n",
      "88.24\n",
      "88.29\n",
      "88.27\n",
      "88.22\n",
      "88.18\n",
      "88.22\n",
      "88.29\n",
      "88.27\n",
      "88.27\n",
      "88.27\n",
      "88.36\n",
      "88.09\n",
      "88.29\n",
      "88.29\n",
      "88.29\n",
      "88.24\n",
      "88.29\n",
      "88.29\n",
      "88.31\n",
      "88.31\n",
      "88.31\n",
      "88.29\n",
      "88.31\n",
      "88.24\n",
      "88.27\n",
      "88.24\n",
      "88.24\n",
      "88.24\n",
      "88.20\n",
      "88.29\n",
      "88.29\n",
      "88.27\n",
      "88.22\n",
      "88.24\n",
      "88.27\n",
      "88.24\n",
      "88.27\n",
      "88.29\n",
      "88.27\n",
      "88.27\n",
      "88.27\n",
      "88.22\n",
      "88.27\n",
      "88.27\n",
      "88.27\n",
      "88.29\n",
      "88.29\n",
      "88.22\n",
      "88.20\n",
      "88.27\n",
      "88.29\n",
      "88.27\n",
      "88.29\n",
      "88.29\n",
      "88.24\n",
      "88.33\n",
      "88.31\n",
      "88.31\n",
      "88.29\n",
      "88.29\n",
      "88.33\n",
      "88.29\n",
      "88.31\n",
      "88.27\n",
      "88.24\n",
      "88.29\n",
      "88.31\n",
      "88.24\n",
      "88.24\n",
      "88.24\n",
      "88.38\n",
      "88.40\n",
      "88.11\n",
      "88.22\n",
      "88.24\n",
      "88.27\n",
      "88.44\n",
      "88.49\n",
      "88.69\n",
      "88.49\n",
      "88.42\n",
      "88.60\n",
      "88.42\n",
      "88.60\n",
      "88.44\n",
      "88.49\n",
      "88.44\n",
      "88.58\n",
      "88.62\n",
      "88.40\n",
      "88.58\n",
      "88.58\n",
      "88.56\n",
      "88.47\n",
      "88.53\n",
      "88.60\n",
      "88.67\n",
      "88.51\n",
      "88.47\n",
      "88.47\n",
      "88.47\n",
      "88.44\n",
      "88.40\n",
      "88.40\n",
      "88.42\n",
      "88.47\n",
      "88.47\n",
      "88.40\n",
      "88.47\n",
      "88.38\n",
      "88.42\n",
      "88.47\n",
      "88.49\n",
      "88.49\n",
      "88.42\n",
      "88.47\n",
      "88.44\n",
      "88.40\n",
      "88.44\n",
      "88.47\n",
      "88.44\n",
      "88.40\n",
      "88.38\n",
      "88.40\n",
      "88.42\n",
      "88.42\n",
      "     fun: 0.9001188786487375\n",
      "     jac: array([ 3.32728530e-06,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "       -2.77360027e-05,  8.54492317e-06,  4.44616111e-05])\n",
      " message: 'Maximum number of iterations has been exceeded.'\n",
      "    nfev: 1466\n",
      "     nit: 1000\n",
      "    njev: 1466\n",
      "  status: 1\n",
      " success: False\n",
      "       x: array([ 0.32122133,  0.        ,  0.        , ..., -0.12491534,\n",
      "        0.03837922,  0.19993573])\n"
     ]
    }
   ],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 1000}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, layers, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        nn_params,\n",
    "                        jac=True,\n",
    "                        method='CG',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "print(res)        \n",
    "# Obtain Theta1 and Theta2 back from nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "active-finance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 0, ..., 7, 2, 7], dtype=uint8)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, _ = nnCostFunction(nn_params, layers, X, y, lambda_)\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
